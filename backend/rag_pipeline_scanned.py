# -*- coding: utf-8 -*-
"""RAG Pipeline Scanned.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M5pF2-JCVglSErpGVlop7pvEU5g76ZKJ

### ğŸ”§ Install Required Libraries
This cell installs the necessary packages.
"""

# Install required libraries

"""### ğŸ§± Environment Setup
This cell imports all core libraries and configures the Google Gemini API key, along with some utility setup for directory and display.

"""

import re
import json
import cv2
import pytesseract
import numpy as np
from PIL import Image
import os
import fitz
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
# from IPython.display import Markdown, display  # Not script compatible
import nest_asyncio
# from google.colab import files  # Not script compatible
from typing import List
from llama_index.schema import Document
import google.generativeai as genai
from llama_index.llms import OpenAI  # We'll use this as a fallback
from llama_index import VectorStoreIndex
from llama_index.core import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms import ChatMessage
from llama_index.prompts import ChatPromptTemplate
from llama_index.postprocessors import SentenceTransformerRerank
from llama_index.retrievers import BM25Retriever
from llama_index.schema import NodeWithScore, QueryBundle, TextNode
from llama_index.retrievers import BaseRetriever
from llama_index.vector_stores import MetadataFilters, ExactMatchFilter
from llama_index.text_splitter import SentenceSplitter
from llama_index.node_parser import SimpleNodeParser

nest_asyncio.apply()

# Set up Google API key for Gemini
GOOGLE_API_KEY = "AIzaSyAyNPT5Kl_Y00dOwlAzQ9Y8mzCQSwZN1F4"  # Replace with your actual API key
os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

# Create a directory for our PDFs if it doesn't exist
# !mkdir -p sample_docs   # Removed for script compatibility

"""### âš™ï¸ RAG Configuration Parameters
All major parameters for chunking, retrieval, reranking, and query expansion are centralized in the `rag_config` dictionary for easier tuning.

"""

# RAG configuration for chunking, retrieval, reranking
rag_config = {
    "fine_chunk_size": 256,
    "fine_chunk_overlap": 20,
    "coarse_chunk_size": 1024,
    "retrieval_top_k": 4,
    "rerank_top_n": 2,
    "num_query_expansions": 3
}

"""### ğŸ“„ Upload PDF
This cell allows the user to upload a PDF file that will be used for building the RAG pipeline.

"""

def upload_pdf():
    """Upload a PDF file and return its path."""
    print("Please select a PDF file to upload:")
    uploaded = files.upload()

    for filename in uploaded.keys():
        if filename.endswith('.pdf'):
            # Save to the sample_docs directory
            pdf_path = os.path.join("sample_docs", filename)

            # Create directory if it doesn't exist
            os.makedirs("sample_docs", exist_ok=True)

            # Save the file
            with open(pdf_path, 'wb') as f:
                f.write(uploaded[filename])

            print(f"PDF saved to {pdf_path}")
            return pdf_path
        else:
            print(f"File {filename} is not a PDF. Please upload a PDF file.")

    return None

# upload your own PDF
pdf_path = upload_pdf()

"""### ğŸ“˜ Load PDF and Extract Text ( PDF Parsing )
Uses PyMuPDF to read each page of the uploaded PDF and convert it into structured LlamaIndex `Document` objects with metadata.

"""

# =======================
# ğŸ“Œ Convert PDF to Images for OCR
# =======================
def pdf_to_images(pdf_path):
    """
    Convert each page of a PDF file into high-resolution images for OCR.
    Returns a list of images, one per page.
    """
    images = []
    with fitz.open(pdf_path) as doc:
        for page_num in range(len(doc)):
            # Extract each page as a high-quality image
            pix = doc[page_num].get_pixmap()
            img = np.array(Image.frombytes("RGB", [pix.width, pix.height], pix.samples))
            images.append(img)
    return images

# =======================
# ğŸ“Œ Preprocess Image for OCR
# =======================
def preprocess_image(img, show_preview=False):
    """
    Preprocess an image for OCR by enhancing contrast and sharpening.
    Uses CLAHE for local contrast normalization and Laplacian filtering for edge enhancement.
    """
    # Convert to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

    # Apply CLAHE to enhance local contrast
    clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(8, 8))
    gray = clahe.apply(gray)

    # Apply a mild sharpening filter to enhance edges
    sharpening_kernel = np.array([
        [0, -1, 0],
        [-1, 4.5, -1],
        [0, -1, 0]
    ])
    gray = cv2.filter2D(gray, -1, sharpening_kernel)

    # Resize for better OCR accuracy (Tesseract works better on larger text)
    scale_percent = 200  # Increase image size by 200%
    width = int(gray.shape[1] * scale_percent / 100)
    height = int(gray.shape[0] * scale_percent / 100)
    gray = cv2.resize(gray, (width, height), interpolation=cv2.INTER_CUBIC)

    # Optionally display the preprocessed image
    if show_preview:
        display(Image.fromarray(gray))

    return gray

# =======================
# ğŸ“Œ Perform OCR on Preprocessed Images
# =======================
def perform_ocr(images):
    """
    Extracts text and bounding box data from a list of preprocessed images.
    Returns extracted text and structured OCR data.
    """
    all_text = []
    all_ocr_data = []
    custom_config = r'--oem 3 -l eng'  # Use Tesseract's LSTM OCR engine

    for gray in images:
        # Extract plain text
        ocr_text = pytesseract.image_to_string(gray, config=custom_config)
        all_text.append(ocr_text)

        # Extract detailed OCR data (bounding boxes, confidence)
        ocr_data = pytesseract.image_to_data(gray, output_type=pytesseract.Output.DICT)
        all_ocr_data.append(ocr_data)

    return all_text, all_ocr_data

# =======================
# ğŸ“Œ Bounding Box Visualization for OCR Validation
# =======================
def visualize_bounding_boxes(original_image, processed_image, ocr_data, confidence_threshold=30):
    """
    Draws bounding boxes on the preprocessed image for verification.
    Only boxes with confidence above the threshold are displayed.
    """
    # Use a color version of the processed image for drawing
    image_copy = cv2.cvtColor(processed_image, cv2.COLOR_GRAY2BGR)

    for i in range(len(ocr_data['text'])):
        # Extract bounding box and confidence information
        x, y, w, h = ocr_data['left'][i], ocr_data['top'][i], ocr_data['width'][i], ocr_data['height'][i]
        conf = int(ocr_data['conf'][i])

        # Draw only high-confidence boxes
        if conf > confidence_threshold:
            image_copy = cv2.rectangle(image_copy, (x, y), (x + w, y + h), (0, 255, 0), 2)
            text = ocr_data['text'][i].strip()
            if text:
                # Overlay the recognized text near the bounding box
                cv2.putText(image_copy, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1, cv2.LINE_AA)

    # Display the image with bounding boxes
    plt.figure(figsize=(15, 20))
    plt.imshow(cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.show()

# =======================
# ğŸ“Œ Document Type Detection
# =======================
def is_scanned_pdf(pdf_path, text_threshold=100):
    """
    Detects if a PDF is scanned or well-structured.
    Returns True if the PDF is likely scanned (low text content), False otherwise.
    """
    with fitz.open(pdf_path) as doc:
        total_text = "".join([page.get_text() for page in doc])

    # Consider it scanned if the total extracted text is below the threshold
    return len(total_text.strip()) < text_threshold

# =======================
# ğŸ“Œ Main Processing Logic
# =======================
# Check if the document is scanned
def process_and_index_pdf(pdf_path):
    """
    Process a scanned or well-structured PDF and create a vector index.
    """
    # Check if the document is scanned
    is_scanned = is_scanned_pdf(pdf_path)

    # Extract text based on document type
    if is_scanned:
        print("ğŸ“ Document Type: Scanned (OCR Required)")
        images = pdf_to_images(pdf_path)
        preprocessed_images = [preprocess_image(img) for img in images]
        ocr_texts, ocr_datasets = perform_ocr(preprocessed_images)
        documents = [Document(text=text) for text in ocr_texts]
    else:
        print("âœ… Document Type: Well-Structured (No OCR Needed)")
        with fitz.open(pdf_path) as doc:
            documents = [Document(text=page.get_text()) for page in doc]

    # Create vector index
    all_nodes = multi_granularity_chunking(documents)
    vector_index = VectorStoreIndex(all_nodes)
    print(f"âœ… Indexed {len(all_nodes)} multi-granularity chunks")

    return vector_index

"""### ğŸ§© Multi-Granularity Chunking
Splits each document into both fine-grained (sentence-level) and coarse-grained (section/page-level) chunks for better retrieval.

"""

# =======================
# ğŸ“Œ Dynamic Multi-Granularity Chunking
# =======================
def multi_granularity_chunking(documents):
    """
    Generate fine, coarse, and logical chunks with dynamic section identification.
    Handles tables, headers, and general sections intelligently.
    """
    fine_nodes = []
    coarse_nodes = []
    logical_nodes = []

    # Fine-grained chunking (sentence-level)
    sentence_splitter = SentenceSplitter(
        chunk_size=rag_config["fine_chunk_size"],
        chunk_overlap=rag_config["fine_chunk_overlap"]
    )
    for i, doc in enumerate(documents):
        nodes = sentence_splitter.get_nodes_from_documents([doc])
        for node in nodes:
            node.metadata["chunk_type"] = "fine"
            node.metadata["page_number"] = i + 1
        fine_nodes.extend(nodes)

    # Coarse-grained chunking (paragraph-level)
    coarse_parser = SimpleNodeParser.from_defaults(
        chunk_size=rag_config["coarse_chunk_size"]
    )
    for i, doc in enumerate(documents):
        nodes = coarse_parser.get_nodes_from_documents([doc])
        for node in nodes:
            node.metadata["chunk_type"] = "coarse"
            node.metadata["page_number"] = i + 1
        coarse_nodes.extend(nodes)

    # Logical chunking (dynamic section headers and table detection)
    for i, doc in enumerate(documents):
        lines = doc.text.split("\n")
        current_chunk = []
        section_title = None
        is_table = False

        for line in lines:
            line_strip = line.strip()

            # Detect dynamic section headers (uppercase, bold, or common financial terms)
            if re.match(r"^[A-Z][A-Z0-9\s-]{3,}$", line_strip) or any(keyword in line_strip.upper() for keyword in [
                "TOTAL ESTIMATED", "FEES WORKSHEET", "ORIGINATION", "MONTHLY PAYMENT", "TABLE", "SUMMARY"
            ]):
                # Flush current chunk if it's not empty
                if current_chunk:
                    logical_nodes.append(TextNode(
                        text="\n".join(current_chunk),
                        metadata={"section": section_title, "chunk_type": "logical", "page_number": i + 1}
                    ))
                    current_chunk = []

                # Start a new section
                section_title = line_strip
                current_chunk.append(line_strip)
                is_table = False  # Reset table flag

            # Detect potential tables (rows with consistent spacing or numeric patterns)
            elif re.match(r"(\d+[\.\d]*)\s+(\d+[\.\d]*)", line_strip) or re.search(r"^\|.+\|$", line_strip):
                # Treat as table if multiple consecutive rows match
                if not is_table:
                    if current_chunk:
                        logical_nodes.append(TextNode(
                            text="\n".join(current_chunk),
                            metadata={"section": section_title, "chunk_type": "logical", "page_number": i + 1}
                        ))
                        current_chunk = []
                    section_title = "TABLE"
                    is_table = True

                current_chunk.append(line_strip)

            # Regular text, part of the current logical section
            elif line_strip:
                current_chunk.append(line_strip)

        # Add the final chunk if it exists
        if current_chunk:
            logical_nodes.append(TextNode(
                text="\n".join(current_chunk),
                metadata={"section": section_title, "chunk_type": "logical", "page_number": i + 1}
            ))

    # Print summary for verification
    print(f"âœ… Final Chunk Counts - Fine: {len(fine_nodes)}, Coarse: {len(coarse_nodes)}, Logical: {len(logical_nodes)}")

    return fine_nodes + coarse_nodes + logical_nodes

"""### ğŸ“š Embedding and Index Construction
Initializes the Gemini LLM and HuggingFace embedding model, then builds a vector index from multi-granularity chunks.

"""

# Set up the custom prompt for the Gemini LLM
custom_prompt = ChatPromptTemplate.from_messages([
    ("system", (
        "You are a highly skilled assistant specializing in analyzing mortgage and loan-related documents, "
        "such as Lender Fee Worksheets and Loan Estimates.\n\n"
        "Your task is to accurately extract and reason over the content retrieved from these documents. "
        "Always rely on the retrieved context only â€” do not assume or hallucinate any values or terms.\n\n"
        "When answering:\n"
        "- Be precise with all numerical values, dates, and percentages.\n"
        "- When totals are requested, check if they need to be calculated from individual components like taxes, insurance, and principal.\n"
        "- If a query implies composition or reasoning (e.g., total payment, remaining balance), compute carefully and explain briefly if needed.\n"
        "- If the information is not in the retrieved content, respond clearly that it was not found.\n"
        "- Use mortgage-specific terminology appropriately and avoid ambiguity.\n\n"
        "You are being used in a legal or financial setting where accuracy and clarity are critical."
    )),
    ("user", "{query_str}")
])

# Initialize the Gemini LLM
# Configure Gemini
genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel('gemini-pro')

# Initialize the LLM (using OpenAI as fallback if Gemini fails)
llm = OpenAI(model="gpt-3.5-turbo", temperature=0.1)

# Initialize the HuggingFace embedding model
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")  # or "intfloat/e5-base-v2"
Settings.embed_model = embed_model

"""### ğŸš€ Index the Uploaded PDF
Calls the full document processing and indexing function to prepare for RAG-based querying.

"""

# =======================
# ğŸ“Œ Run the Indexing
# =======================
index = process_and_index_pdf(pdf_path)

"""### ğŸ” Reranking Demonstration
Shows the effect of reranking retrieval results using a cross-encoder model for better relevance ordering.

"""

# Create a reranker
def rerank_results(nodes, query, top_n=None):
    """Rerank retrieved nodes using configurable top_n."""
    reranker = SentenceTransformerRerank(
        model="cross-encoder/ms-marco-MiniLM-L-12-v2",
        top_n=top_n or rag_config["rerank_top_n"]
    )

    reranked_nodes = reranker.postprocess_nodes(nodes, query_str=query)
    return reranked_nodes

# Function to demonstrate the reranking process
def demonstrate_reranking(index, query, top_k=4):
    """Demonstrate the reranking process on retrieval results."""
    # First retrieve more nodes than we need
    retriever = index.as_retriever(similarity_top_k=top_k)
    nodes = retriever.retrieve(query)

    print(f"Query: {query}")
    print("\nOriginal Retrieval Order:")
    for i, node in enumerate(nodes):
        print(f"{i+1}. (Score: {node.score:.4f}) - {node.get_text()[:100]}...")

    # Now rerank them
    reranked_nodes = rerank_results(nodes, query, top_n=rag_config["rerank_top_n"])


    print("\nAfter Reranking:")
    for i, node in enumerate(reranked_nodes):
        print(f"{i+1}. (Score: {node.score:.4f}) - {node.get_text()[:100]}...")

    # Create comparison dataframe
    results = []

    # Original ranking
    for i, node in enumerate(nodes):
        results.append({
            "Stage": "Original Retrieval",
            "Rank": i + 1,
            "Score": node.score,
            "Content": node.get_text()[:150] + "...",
            "Page": node.metadata.get("page_number", "Unknown")
        })

    # Reranked
    for i, node in enumerate(reranked_nodes):
        results.append({
            "Stage": "After Reranking",
            "Rank": i + 1,
            "Score": node.score,
            "Content": node.get_text()[:150] + "...",
            "Page": node.metadata.get("page_number", "Unknown")
        })

    results_df = pd.DataFrame(results)
    display(results_df)

    return results_df
# Example usage:
reranking_demo = demonstrate_reranking(index, "What is the name and return address?", top_k=4)

"""### ğŸ§  Full RAG Pipeline Builder
Constructs the complete Retrieval-Augmented Generation engine with hybrid retrieval (semantic + keyword), query expansion, and reranking.

"""

def build_rag_pipeline(index):
    """Build a RAG pipeline with hybrid retrieval, query expansion, and reranking."""

    # Get all nodes from the index's docstore
    nodes = list(index.docstore.docs.values())
    num_nodes = len(nodes)

    # Set top_k based on configuration
    safe_top_k = min(rag_config["retrieval_top_k"], max(1, num_nodes))
    print(f"Index contains {num_nodes} nodes, using top_k={safe_top_k}")

    # Step 1: Define filters based on chunk types
    filter_by_fine_type = MetadataFilters(filters=[ExactMatchFilter(key="chunk_type", value="fine")])
    filter_by_coarse_type = MetadataFilters(filters=[ExactMatchFilter(key="chunk_type", value="coarse")])
    filter_by_logical_type = MetadataFilters(filters=[ExactMatchFilter(key="chunk_type", value="logical")])

    # Step 2: Create individual retrievers
    fine_retriever = index.as_retriever(similarity_top_k=safe_top_k, filters=filter_by_fine_type)
    coarse_retriever = index.as_retriever(similarity_top_k=safe_top_k, filters=filter_by_coarse_type)
    logical_retriever = index.as_retriever(similarity_top_k=safe_top_k, filters=filter_by_logical_type)
    bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=safe_top_k)

    # Step 3: Hybrid Retriever
    class HybridRetriever(BaseRetriever):
        def __init__(self, fine_retriever, coarse_retriever, logical_retriever, bm25_retriever, top_k):
            self.fine_retriever = fine_retriever
            self.coarse_retriever = coarse_retriever
            self.logical_retriever = logical_retriever
            self.bm25_retriever = bm25_retriever
            self.top_k = top_k
            super().__init__()

        def _retrieve(self, query_bundle, **kwargs):
            fine_nodes = self.fine_retriever.retrieve(query_bundle)
            coarse_nodes = self.coarse_retriever.retrieve(query_bundle)
            logical_nodes = self.logical_retriever.retrieve(query_bundle)
            bm25_nodes = self.bm25_retriever.retrieve(query_bundle)

            # Combine all retrieved nodes
            all_nodes = list(fine_nodes) + list(coarse_nodes) + list(logical_nodes) + list(bm25_nodes)

            # Remove duplicates by node_id
            unique_nodes = {}
            for node in all_nodes:
                if node.node_id not in unique_nodes:
                    unique_nodes[node.node_id] = node

            # Sort nodes by score, highest first
            sorted_nodes = sorted(
                unique_nodes.values(),
                key=lambda x: x.score if hasattr(x, "score") and x.score else 0.0,
                reverse=True
            )

            # Return top_k results
            return sorted_nodes[:self.top_k]

    hybrid_retriever = HybridRetriever(
        fine_retriever=fine_retriever,
        coarse_retriever=coarse_retriever,
        logical_retriever=logical_retriever,
        bm25_retriever=bm25_retriever,
        top_k=safe_top_k
    )

    # Step 4: Reranker
    node_postprocessors = []
    if num_nodes > 1:
        reranker = SentenceTransformerRerank(
            model="cross-encoder/ms-marco-MiniLM-L-12-v2",
            top_n=min(rag_config["rerank_top_n"], num_nodes)
        )
        node_postprocessors.append(reranker)

    # Step 5: Query Expansion with Fusion Retriever
    fusion_retriever = QueryFusionRetriever(
        retrievers=[hybrid_retriever],
        llm=llm,
        num_queries=rag_config["num_query_expansions"],
        similarity_top_k=safe_top_k,
        mode="reciprocal_rerank"
    )

    # Step 6: Final Query Engine
    from llama_index.query_engine import RetrieverQueryEngine
    query_engine = RetrieverQueryEngine.from_args(
        retriever=fusion_retriever,
        llm=llm,
        node_postprocessors=node_postprocessors
    )

    print(f"âœ… RAG Pipeline built successfully with {num_nodes} nodes.")

    def run_query_with_reranking(query_text, top_k=4):
        query_bundle = QueryBundle(query_str=query_text)
        nodes = hybrid_retriever._retrieve(query_bundle)
        reranked_nodes = reranker.postprocess_nodes(nodes, query_str=query_text) if reranker else nodes

        # Prepare DataFrame for comparison
        results = []

        # Combined Results for Comparison
        for i, node in enumerate(nodes):
            results.append({
                "Stage": "Original Retrieval",
                "Rank": i + 1,
                "Score": node.score,
                "Content": node.get_text()[:150] + "...",
                "Page": node.metadata.get("page_number", "Unknown")
            })

        for i, node in enumerate(reranked_nodes):
            results.append({
                "Stage": "After Reranking",
                "Rank": i + 1,
                "Score": node.score,
                "Content": node.get_text()[:150] + "...",
                "Page": node.metadata.get("page_number", "Unknown")
            })

        # Display the results in a clean table
        results_df = pd.DataFrame(results)
        display(results_df)

    return query_engine, run_query_with_reranking

# Rebuild the RAG pipeline
rag_engine, run_query_with_reranking = build_rag_pipeline(index)

# Example usage of the integrated reranking demonstration
run_query_with_reranking("What is the name and return address?", top_k=4)

"""### ğŸ” Run a Sample Query
This cell queries the final RAG engine to answer a question using the processed PDF content.

"""

# ğŸ§° Imports
import ipywidgets as widgets
from IPython.display import display, clear_output, HTML

# ğŸ“‚ Ensure the index is processed correctly
index = process_and_index_pdf(pdf_path)
rag_engine, run_query_with_reranking = build_rag_pipeline(index)

# ğŸ“‹ Preset prompts
preset_prompts = {
    "Who is the mortgagor?": "Who is the mortgagor?",
    "Borrower's owes": "How much does the borrower owe the lender?",
    "Uniform Covenants": "What are Uniform Covenants?",
    "Parcel Identifier number": "What is the parcel identifier number?",
    "Document number": "What is the document number?"
}

# ğŸ”½ Dropdown for sample prompts
prompt_dropdown = widgets.Dropdown(
    options=["Select a sample prompt"] + list(preset_prompts.keys()),
    description='Examples:',
    layout=widgets.Layout(width='90%')
)

# ğŸ“ Multiline text input
query_input = widgets.Textarea(
    placeholder='Type or select a query...',
    description='Query:',
    layout=widgets.Layout(width='90%', height='120px')
)

# ğŸ”˜ Submit button
submit_button = widgets.Button(
    description='Submit',
    button_style='success',
    tooltip='Submit your query',
    icon='search'
)

# ğŸ“¦ Output box
output_box = widgets.Output()

# ğŸ§  Define query execution function
def run_query(query_text):
    try:
        # Strip any leading or trailing whitespace
        cleaned_query = query_text.strip()

        # Run the query
        response = rag_engine.query(cleaned_query).response

        # Format the response in a styled box with black text
        formatted_response = f"""
        <div style="
            background-color: #f9f9f9;
            border-left: 6px solid #007bff;
            padding: 15px;
            margin-bottom: 15px;
            border-radius: 8px;
            font-family: Arial, sans-serif;
            color: #000000;  /* Ensure text is black */
        ">
        <h3>ğŸ“ Query:</h3>
        <p><strong>{cleaned_query}</strong></p>
        <hr>
        <h3>ğŸ“„ Response:</h3>
        <p>{response}</p>
        </div>
        """

        # Display formatted HTML
        with output_box:
            clear_output()
            display(HTML(formatted_response))

    except Exception as e:
        # Display error in styled box
        with output_box:
            clear_output()
            display(HTML(f"""
            <div style="
                background-color: #f8d7da;
                border-left: 6px solid #dc3545;
                padding: 15px;
                margin-bottom: 15px;
                border-radius: 8px;
                font-family: Arial, sans-serif;
                color: #000000;  /* Ensure text is black */
            ">
            <h3>âŒ Error:</h3>
            <p>{str(e)}</p>
            </div>
            """))

# ğŸ§© Handle prompt selection
def on_prompt_change(change):
    if change.new in preset_prompts:
        query_input.value = preset_prompts[change.new]

prompt_dropdown.observe(on_prompt_change, names='value')

# ğŸš€ Handle query submission
def on_submit_click(b):
    run_query(query_input.value.strip())

submit_button.on_click(on_submit_click)

# ğŸ¯ Display the interactive UI
display(widgets.VBox([
    prompt_dropdown,
    query_input,
    submit_button,
    output_box
]))


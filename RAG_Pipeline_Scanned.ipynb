{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0Uuxr7iwBnx"
      },
      "source": [
        "### üîß Install Required Libraries\n",
        "This cell installs the necessary packages.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mWwPH4IWhZ3d"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q llama-index-llms-google-genai llama-index pymupdf\n",
        "!pip install -q llama-index-embeddings-huggingface\n",
        "!pip install nest_asyncio\n",
        "!pip install llama-index-retrievers-bm25\n",
        "!pip install -q ipywidgets\n",
        "!apt install tesseract-ocr\n",
        "!pip install pymupdf pytesseract opencv-python pillow\n",
        "!pip install sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUllT2HgwH0c"
      },
      "source": [
        "### üß± Environment Setup\n",
        "This cell imports all core libraries and configures the Google Gemini API key, along with some utility setup for directory and display.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyknyHvxhltC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import cv2\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import fitz\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Markdown, display\n",
        "import nest_asyncio\n",
        "from google.colab import files\n",
        "from llama_index.core import Document\n",
        "from typing import List\n",
        "from llama_index.core.schema import Document, TextNode\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.prompts import ChatPromptTemplate\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "from llama_index.core.retrievers import BaseRetriever\n",
        "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
        "from llama_index.core.retrievers import QueryFusionRetriever\n",
        "from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter\n",
        "from llama_index.core.text_splitter import SentenceSplitter\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Set up Google API key for Gemini\n",
        "GOOGLE_API_KEY = \"AIzaSyAyNPT5Kl_Y00dOwlAzQ9Y8mzCQSwZN1F4\"  # Replace with your actual API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "# Create a directory for our PDFs if it doesn't exist\n",
        "!mkdir -p sample_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsmgVyOHwKTt"
      },
      "source": [
        "### ‚öôÔ∏è RAG Configuration Parameters\n",
        "All major parameters for chunking, retrieval, reranking, and query expansion are centralized in the `rag_config` dictionary for easier tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTERkVFep0w1"
      },
      "outputs": [],
      "source": [
        "# RAG configuration for chunking, retrieval, reranking\n",
        "rag_config = {\n",
        "    \"fine_chunk_size\": 256,\n",
        "    \"fine_chunk_overlap\": 20,\n",
        "    \"coarse_chunk_size\": 1024,\n",
        "    \"retrieval_top_k\": 4,\n",
        "    \"rerank_top_n\": 2,\n",
        "    \"num_query_expansions\": 3\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK9-bLN5wMi5"
      },
      "source": [
        "### üìÑ Upload PDF\n",
        "This cell allows the user to upload a PDF file that will be used for building the RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA8EFEu6hlph"
      },
      "outputs": [],
      "source": [
        "def upload_pdf():\n",
        "    \"\"\"Upload a PDF file and return its path.\"\"\"\n",
        "    print(\"Please select a PDF file to upload:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        if filename.endswith('.pdf'):\n",
        "            # Save to the sample_docs directory\n",
        "            pdf_path = os.path.join(\"sample_docs\", filename)\n",
        "\n",
        "            # Create directory if it doesn't exist\n",
        "            os.makedirs(\"sample_docs\", exist_ok=True)\n",
        "\n",
        "            # Save the file\n",
        "            with open(pdf_path, 'wb') as f:\n",
        "                f.write(uploaded[filename])\n",
        "\n",
        "            print(f\"PDF saved to {pdf_path}\")\n",
        "            return pdf_path\n",
        "        else:\n",
        "            print(f\"File {filename} is not a PDF. Please upload a PDF file.\")\n",
        "\n",
        "    return None\n",
        "\n",
        "# upload your own PDF\n",
        "pdf_path = upload_pdf()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìò Load PDF and Extract Text ( PDF Parsing )\n",
        "Uses PyMuPDF to read each page of the uploaded PDF and convert it into structured LlamaIndex `Document` objects with metadata.\n"
      ],
      "metadata": {
        "id": "R-VXtzl3mA9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# üìå Convert PDF to Images for OCR\n",
        "# =======================\n",
        "def pdf_to_images(pdf_path):\n",
        "    \"\"\"\n",
        "    Convert each page of a PDF file into high-resolution images for OCR.\n",
        "    Returns a list of images, one per page.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        for page_num in range(len(doc)):\n",
        "            # Extract each page as a high-quality image\n",
        "            pix = doc[page_num].get_pixmap()\n",
        "            img = np.array(Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples))\n",
        "            images.append(img)\n",
        "    return images\n",
        "\n",
        "# =======================\n",
        "# üìå Preprocess Image for OCR\n",
        "# =======================\n",
        "def preprocess_image(img, show_preview=False):\n",
        "    \"\"\"\n",
        "    Preprocess an image for OCR by enhancing contrast and sharpening.\n",
        "    Uses CLAHE for local contrast normalization and Laplacian filtering for edge enhancement.\n",
        "    \"\"\"\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Apply CLAHE to enhance local contrast\n",
        "    clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(8, 8))\n",
        "    gray = clahe.apply(gray)\n",
        "\n",
        "    # Apply a mild sharpening filter to enhance edges\n",
        "    sharpening_kernel = np.array([\n",
        "        [0, -1, 0],\n",
        "        [-1, 4.5, -1],\n",
        "        [0, -1, 0]\n",
        "    ])\n",
        "    gray = cv2.filter2D(gray, -1, sharpening_kernel)\n",
        "\n",
        "    # Resize for better OCR accuracy (Tesseract works better on larger text)\n",
        "    scale_percent = 200  # Increase image size by 200%\n",
        "    width = int(gray.shape[1] * scale_percent / 100)\n",
        "    height = int(gray.shape[0] * scale_percent / 100)\n",
        "    gray = cv2.resize(gray, (width, height), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    # Optionally display the preprocessed image\n",
        "    if show_preview:\n",
        "        display(Image.fromarray(gray))\n",
        "\n",
        "    return gray\n",
        "\n",
        "# =======================\n",
        "# üìå Perform OCR on Preprocessed Images\n",
        "# =======================\n",
        "def perform_ocr(images):\n",
        "    \"\"\"\n",
        "    Extracts text and bounding box data from a list of preprocessed images.\n",
        "    Returns extracted text and structured OCR data.\n",
        "    \"\"\"\n",
        "    all_text = []\n",
        "    all_ocr_data = []\n",
        "    custom_config = r'--oem 3 -l eng'  # Use Tesseract's LSTM OCR engine\n",
        "\n",
        "    for gray in images:\n",
        "        # Extract plain text\n",
        "        ocr_text = pytesseract.image_to_string(gray, config=custom_config)\n",
        "        all_text.append(ocr_text)\n",
        "\n",
        "        # Extract detailed OCR data (bounding boxes, confidence)\n",
        "        ocr_data = pytesseract.image_to_data(gray, output_type=pytesseract.Output.DICT)\n",
        "        all_ocr_data.append(ocr_data)\n",
        "\n",
        "    return all_text, all_ocr_data\n",
        "\n",
        "# =======================\n",
        "# üìå Bounding Box Visualization for OCR Validation\n",
        "# =======================\n",
        "def visualize_bounding_boxes(original_image, processed_image, ocr_data, confidence_threshold=30):\n",
        "    \"\"\"\n",
        "    Draws bounding boxes on the preprocessed image for verification.\n",
        "    Only boxes with confidence above the threshold are displayed.\n",
        "    \"\"\"\n",
        "    # Use a color version of the processed image for drawing\n",
        "    image_copy = cv2.cvtColor(processed_image, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    for i in range(len(ocr_data['text'])):\n",
        "        # Extract bounding box and confidence information\n",
        "        x, y, w, h = ocr_data['left'][i], ocr_data['top'][i], ocr_data['width'][i], ocr_data['height'][i]\n",
        "        conf = int(ocr_data['conf'][i])\n",
        "\n",
        "        # Draw only high-confidence boxes\n",
        "        if conf > confidence_threshold:\n",
        "            image_copy = cv2.rectangle(image_copy, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            text = ocr_data['text'][i].strip()\n",
        "            if text:\n",
        "                # Overlay the recognized text near the bounding box\n",
        "                cv2.putText(image_copy, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1, cv2.LINE_AA)\n",
        "\n",
        "    # Display the image with bounding boxes\n",
        "    plt.figure(figsize=(15, 20))\n",
        "    plt.imshow(cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# =======================\n",
        "# üìå Document Type Detection\n",
        "# =======================\n",
        "def is_scanned_pdf(pdf_path, text_threshold=100):\n",
        "    \"\"\"\n",
        "    Detects if a PDF is scanned or well-structured.\n",
        "    Returns True if the PDF is likely scanned (low text content), False otherwise.\n",
        "    \"\"\"\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        total_text = \"\".join([page.get_text() for page in doc])\n",
        "\n",
        "    # Consider it scanned if the total extracted text is below the threshold\n",
        "    return len(total_text.strip()) < text_threshold\n",
        "\n",
        "# =======================\n",
        "# üìå Main Processing Logic (Optimized)\n",
        "# =======================\n",
        "def process_and_index_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Process a scanned or well-structured PDF and create a vector index with improved handling for structured PDFs.\n",
        "    \"\"\"\n",
        "    # Check if the document is scanned\n",
        "    is_scanned = is_scanned_pdf(pdf_path)\n",
        "\n",
        "    # Extract text based on document type\n",
        "    if is_scanned:\n",
        "        print(\"üìù Document Type: Scanned (OCR Required)\")\n",
        "        images = pdf_to_images(pdf_path)\n",
        "        preprocessed_images = [preprocess_image(img) for img in images]\n",
        "        ocr_texts, ocr_datasets = perform_ocr(preprocessed_images)\n",
        "        documents = [Document(text=text) for text in ocr_texts]\n",
        "    else:\n",
        "        print(\"‚úÖ Document Type: Well-Structured (No OCR Needed)\")\n",
        "        documents = []\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            for i, page in enumerate(doc):\n",
        "                # Extract raw text from each page\n",
        "                text = page.get_text()\n",
        "\n",
        "                # Preserve critical newlines for logical chunking\n",
        "                text = \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
        "\n",
        "                # Create Document object with metadata\n",
        "                documents.append(Document(\n",
        "                    text=text,\n",
        "                    metadata={\n",
        "                        \"file_name\": os.path.basename(pdf_path),\n",
        "                        \"page_number\": i + 1,\n",
        "                        \"total_pages\": len(doc)\n",
        "                    }\n",
        "                ))\n",
        "\n",
        "    # Create vector index\n",
        "    all_nodes = multi_granularity_chunking(documents, pdf_path=pdf_path)\n",
        "    vector_index = VectorStoreIndex(all_nodes)\n",
        "    print(f\"‚úÖ Indexed {len(all_nodes)} multi-granularity chunks\")\n",
        "\n",
        "    return vector_index\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fCnNYM_rl5-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cpiw3gawXoQ"
      },
      "source": [
        "### üß© Multi-Granularity Chunking\n",
        "Splits each document into both fine-grained (sentence-level) and coarse-grained (section/page-level) chunks for better retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKaRrzKNmb0N"
      },
      "outputs": [],
      "source": [
        "# =======================\n",
        "# üìå Multi-Granularity Chunking (Optimized for Structured and Scanned PDFs)\n",
        "# =======================\n",
        "def multi_granularity_chunking(documents, pdf_path, text_threshold=100):\n",
        "    \"\"\"\n",
        "    Generate fine, coarse, and logical chunks with separate strategies for structured and scanned PDFs.\n",
        "    - Retains precise section titles for structured PDFs.\n",
        "    - Simplifies chunking for scanned PDFs to reduce noise.\n",
        "    \"\"\"\n",
        "    # Check if the document is scanned\n",
        "    is_scanned = is_scanned_pdf(pdf_path, text_threshold=text_threshold)\n",
        "\n",
        "    fine_nodes = []\n",
        "    coarse_nodes = []\n",
        "    logical_nodes = []\n",
        "\n",
        "    # Fine-grained chunking (sentence-level)\n",
        "    sentence_splitter = SentenceSplitter(\n",
        "        chunk_size=rag_config[\"fine_chunk_size\"],\n",
        "        chunk_overlap=rag_config[\"fine_chunk_overlap\"]\n",
        "    )\n",
        "    for i, doc in enumerate(documents):\n",
        "        nodes = sentence_splitter.get_nodes_from_documents([doc])\n",
        "        for node in nodes:\n",
        "            node.metadata[\"chunk_type\"] = \"fine\"\n",
        "            node.metadata[\"page_number\"] = i + 1\n",
        "        fine_nodes.extend(nodes)\n",
        "\n",
        "    # Coarse-grained chunking (paragraph-level)\n",
        "    coarse_parser = SimpleNodeParser.from_defaults(\n",
        "        chunk_size=rag_config[\"coarse_chunk_size\"]\n",
        "    )\n",
        "    for i, doc in enumerate(documents):\n",
        "        nodes = coarse_parser.get_nodes_from_documents([doc])\n",
        "        for node in nodes:\n",
        "            node.metadata[\"chunk_type\"] = \"coarse\"\n",
        "            node.metadata[\"page_number\"] = i + 1\n",
        "        coarse_nodes.extend(nodes)\n",
        "\n",
        "    # Logical chunking (Structured PDFs)\n",
        "    if not is_scanned:\n",
        "        for doc in documents:\n",
        "            lines = doc.text.split(\"\\n\")\n",
        "            current_chunk = []\n",
        "            collecting = False\n",
        "            section_title = None\n",
        "\n",
        "            for line in lines:\n",
        "                line_strip = line.strip()\n",
        "\n",
        "                # Detect known section headers\n",
        "                if any(anchor in line_strip.upper() for anchor in [\n",
        "                    \"TOTAL ESTIMATED MONTHLY PAYMENT\",\n",
        "                    \"FEES WORKSHEET\",\n",
        "                    \"ORIGINATION CHARGES\",\n",
        "                    \"OTHER CHARGES\"\n",
        "                ]):\n",
        "                    if current_chunk:\n",
        "                        chunk_text = \"\\n\".join(current_chunk)\n",
        "                        logical_nodes.append(TextNode(text=chunk_text, metadata={\"section\": section_title, \"chunk_type\": \"logical\"}))\n",
        "                        current_chunk = []\n",
        "\n",
        "                    collecting = True\n",
        "                    section_title = line_strip\n",
        "                    current_chunk = [line_strip]\n",
        "                    continue\n",
        "\n",
        "                if collecting:\n",
        "                    if line_strip == \"\" and current_chunk:\n",
        "                        chunk_text = \"\\n\".join(current_chunk)\n",
        "                        logical_nodes.append(TextNode(text=chunk_text, metadata={\"section\": section_title, \"chunk_type\": \"logical\"}))\n",
        "                        current_chunk = []\n",
        "                        collecting = False\n",
        "                    else:\n",
        "                        current_chunk.append(line_strip)\n",
        "\n",
        "            # Add remaining chunk\n",
        "            if current_chunk:\n",
        "                chunk_text = \"\\n\".join(current_chunk)\n",
        "                logical_nodes.append(TextNode(text=chunk_text, metadata={\"section\": section_title, \"chunk_type\": \"logical\"}))\n",
        "\n",
        "    # Logical chunking (Scanned PDFs)\n",
        "    else:\n",
        "        for doc in documents:\n",
        "            lines = doc.text.split(\"\\n\")\n",
        "            current_chunk = []\n",
        "            for line in lines:\n",
        "                line_strip = line.strip()\n",
        "                if line_strip:\n",
        "                    current_chunk.append(line_strip)\n",
        "            if current_chunk:\n",
        "                logical_nodes.append(TextNode(\n",
        "                    text=\"\\n\".join(current_chunk),\n",
        "                    metadata={\"chunk_type\": \"logical\"}\n",
        "                ))\n",
        "\n",
        "    # Print summary for verification\n",
        "    print(f\"‚úÖ Final Chunk Counts - Fine: {len(fine_nodes)}, Coarse: {len(coarse_nodes)}, Logical: {len(logical_nodes)}\")\n",
        "\n",
        "    return fine_nodes + coarse_nodes + logical_nodes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P27jpGZ1waOQ"
      },
      "source": [
        "### üìö Embedding and Index Construction\n",
        "Initializes the Gemini LLM and HuggingFace embedding model, then builds a vector index from multi-granularity chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPOWFNl-hlhX",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Set up the custom prompt for the Gemini LLM\n",
        "custom_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"You are a highly skilled assistant specializing in analyzing mortgage and loan-related documents, \"\n",
        "        \"such as Lender Fee Worksheets and Loan Estimates.\\n\\n\"\n",
        "        \"Your task is to accurately extract and reason over the content retrieved from these documents. \"\n",
        "        \"Always rely on the retrieved context only ‚Äî do not assume or hallucinate any values or terms.\\n\\n\"\n",
        "        \"When answering:\\n\"\n",
        "        \"- Be precise with all numerical values, dates, and percentages.\\n\"\n",
        "        \"- When totals are requested, check if they need to be calculated from individual components like taxes, insurance, and principal.\\n\"\n",
        "        \"- If a query implies composition or reasoning (e.g., total payment, remaining balance), compute carefully and explain briefly if needed.\\n\"\n",
        "        \"- If the information is not in the retrieved content, respond clearly that it was not found.\\n\"\n",
        "        \"- Use mortgage-specific terminology appropriately and avoid ambiguity.\\n\\n\"\n",
        "        \"You are being used in a legal or financial setting where accuracy and clarity are critical.\"\n",
        "    )),\n",
        "    (\"user\", \"{query_str}\")\n",
        "])\n",
        "\n",
        "# Initialize the Gemini LLM\n",
        "llm = GoogleGenAI(\n",
        "    model=\"models/gemini-2.0-flash\",\n",
        "    prompt=custom_prompt,\n",
        "    max_output_tokens=1024  # Adjust based on your use case\n",
        ")\n",
        "\n",
        "# Initialize the HuggingFace embedding model\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")  # or \"intfloat/e5-base-v2\"\n",
        "Settings.embed_model = embed_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRnAO5IcwjHx"
      },
      "source": [
        "### üöÄ Index the Uploaded PDF\n",
        "Calls the full document processing and indexing function to prepare for RAG-based querying.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQPaeltMhlet",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# =======================\n",
        "# üìå Run the Indexing\n",
        "# =======================\n",
        "index = process_and_index_pdf(pdf_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JT8UGiiw36W"
      },
      "source": [
        "### üß† Full RAG Pipeline Builder\n",
        "Constructs the complete Retrieval-Augmented Generation engine with hybrid retrieval (semantic + keyword), query expansion, and reranking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLAy6xLahlUS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def build_rag_pipeline(index):\n",
        "    \"\"\"Build a RAG pipeline with hybrid retrieval, query expansion, and reranking.\"\"\"\n",
        "\n",
        "    # Get all nodes from the index's docstore\n",
        "    nodes = list(index.docstore.docs.values())\n",
        "    num_nodes = len(nodes)\n",
        "\n",
        "    # Set top_k based on configuration\n",
        "    safe_top_k = min(rag_config[\"retrieval_top_k\"], max(1, num_nodes))\n",
        "    print(f\"Index contains {num_nodes} nodes, using top_k={safe_top_k}\")\n",
        "\n",
        "    # Step 1: Define Filters for Each Retrieval Method\n",
        "    filter_by_fine_type = MetadataFilters(filters=[ExactMatchFilter(key=\"chunk_type\", value=\"fine\")])\n",
        "    filter_by_coarse_type = MetadataFilters(filters=[ExactMatchFilter(key=\"chunk_type\", value=\"coarse\")])\n",
        "    filter_by_page_level = MetadataFilters(filters=[ExactMatchFilter(key=\"chunk_type\", value=\"page\")])\n",
        "\n",
        "    # Step 2: Create Individual Retrievers\n",
        "    vector_retriever = index.as_retriever(similarity_top_k=safe_top_k, filters=filter_by_fine_type)\n",
        "    bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=safe_top_k)\n",
        "    page_retriever = index.as_retriever(similarity_top_k=safe_top_k, filters=filter_by_page_level)\n",
        "\n",
        "    # Step 3: Hybrid Retriever for Combining Methods\n",
        "    class HybridRetriever(BaseRetriever):\n",
        "        def __init__(self, vector_retriever, bm25_retriever, page_retriever, top_k):\n",
        "            self.vector_retriever = vector_retriever\n",
        "            self.bm25_retriever = bm25_retriever\n",
        "            self.page_retriever = page_retriever\n",
        "            self.top_k = top_k\n",
        "            super().__init__()\n",
        "\n",
        "        def _retrieve(self, query_bundle, **kwargs):\n",
        "            vector_nodes = self.vector_retriever.retrieve(query_bundle)\n",
        "            bm25_nodes = self.bm25_retriever.retrieve(query_bundle)\n",
        "            page_nodes = self.page_retriever.retrieve(query_bundle)\n",
        "\n",
        "            # Combine all retrieved nodes\n",
        "            all_nodes = list(vector_nodes) + list(bm25_nodes) + list(page_nodes)\n",
        "\n",
        "            # Remove duplicates by node_id\n",
        "            unique_nodes = {}\n",
        "            for node in all_nodes:\n",
        "                if node.node_id not in unique_nodes:\n",
        "                    unique_nodes[node.node_id] = node\n",
        "\n",
        "            # Sort nodes by score, highest first\n",
        "            sorted_nodes = sorted(\n",
        "                unique_nodes.values(),\n",
        "                key=lambda x: x.score if hasattr(x, \"score\") and x.score else 0.0,\n",
        "                reverse=True\n",
        "            )\n",
        "\n",
        "            # Return top_k results\n",
        "            return sorted_nodes[:self.top_k]\n",
        "\n",
        "    # Initialize Hybrid Retriever\n",
        "    hybrid_retriever = HybridRetriever(\n",
        "        vector_retriever=vector_retriever,\n",
        "        bm25_retriever=bm25_retriever,\n",
        "        page_retriever=page_retriever,\n",
        "        top_k=safe_top_k\n",
        "    )\n",
        "\n",
        "    # Step 4: Reranker (Optional)\n",
        "    node_postprocessors = []\n",
        "    if num_nodes > 1:\n",
        "        reranker = SentenceTransformerRerank(\n",
        "            model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\",\n",
        "            top_n=min(rag_config[\"rerank_top_n\"], num_nodes)\n",
        "        )\n",
        "        node_postprocessors.append(reranker)\n",
        "\n",
        "    # Step 5: Query Expansion with Fusion Retriever\n",
        "    fusion_retriever = QueryFusionRetriever(\n",
        "        retrievers=[hybrid_retriever],\n",
        "        llm=llm,\n",
        "        num_queries=rag_config[\"num_query_expansions\"],\n",
        "        similarity_top_k=safe_top_k,\n",
        "        mode=\"reciprocal_rerank\"\n",
        "    )\n",
        "\n",
        "    # Step 6: Final Query Engine\n",
        "    query_engine = RetrieverQueryEngine.from_args(\n",
        "        retriever=fusion_retriever,\n",
        "        llm=llm,\n",
        "        node_postprocessors=node_postprocessors\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Hybrid RAG Pipeline built successfully with {num_nodes} nodes.\")\n",
        "\n",
        "    def run_query_with_reranking(query_text, top_k=4):\n",
        "        query_bundle = QueryBundle(query_str=query_text)\n",
        "        nodes = hybrid_retriever._retrieve(query_bundle)\n",
        "        reranked_nodes = reranker.postprocess_nodes(nodes, query_str=query_text) if reranker else nodes\n",
        "\n",
        "        # Prepare DataFrame for comparison\n",
        "        results = []\n",
        "\n",
        "        # Combined Results for Comparison\n",
        "        for i, node in enumerate(nodes):\n",
        "            results.append({\n",
        "                \"Stage\": \"Original Retrieval\",\n",
        "                \"Rank\": i + 1,\n",
        "                \"Score\": node.score,\n",
        "                \"Content\": node.get_text()[:150] + \"...\",\n",
        "                \"Page\": node.metadata.get(\"page_number\", \"Unknown\")\n",
        "            })\n",
        "\n",
        "        for i, node in enumerate(reranked_nodes):\n",
        "            results.append({\n",
        "                \"Stage\": \"After Reranking\",\n",
        "                \"Rank\": i + 1,\n",
        "                \"Score\": node.score,\n",
        "                \"Content\": node.get_text()[:150] + \"...\",\n",
        "                \"Page\": node.metadata.get(\"page_number\", \"Unknown\")\n",
        "            })\n",
        "\n",
        "        # Display the results in a clean table\n",
        "        results_df = pd.DataFrame(results)\n",
        "        display(results_df)\n",
        "\n",
        "    return query_engine, run_query_with_reranking\n",
        "\n",
        "# Rebuild the RAG pipeline\n",
        "rag_engine, run_query_with_reranking = build_rag_pipeline(index)\n",
        "\n",
        "# Example usage of the integrated reranking demonstration\n",
        "run_query_with_reranking(\"What is the total estimated monthly payment?\", top_k=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHXrXah0xI3Z"
      },
      "source": [
        "### üîç Run a Sample Query\n",
        "This cell queries the final RAG engine to answer a question using the processed PDF content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gV3gPW2j2zF"
      },
      "outputs": [],
      "source": [
        "# üß∞ Imports\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "\n",
        "# üìÇ Ensure the index is processed correctly\n",
        "index = process_and_index_pdf(pdf_path)\n",
        "rag_engine, run_query_with_reranking = build_rag_pipeline(index)\n",
        "\n",
        "# üìã Preset prompts\n",
        "preset_prompts = {\n",
        "    \"Who is the mortgagor?\": \"Who is the mortgagor in the document?\",\n",
        "    \"Borrower's owes\": \"How much does the borrower owes the lender?\",\n",
        "    \"Uniform Covenants\": \"What are the Uniform Covenants in detail?\",\n",
        "    \"Parcel Identifier number\": \"What is the parcel identifier number?\",\n",
        "    \"Document number\": \"What is the document number?\",\n",
        "    \"what are escrow items?\": \"what are escrow items?\"\n",
        "\n",
        "}\n",
        "\n",
        "# üîΩ Dropdown for sample prompts\n",
        "prompt_dropdown = widgets.Dropdown(\n",
        "    options=[\"Select a sample prompt\"] + list(preset_prompts.keys()),\n",
        "    description='Examples:',\n",
        "    layout=widgets.Layout(width='90%')\n",
        ")\n",
        "\n",
        "# üìù Multiline text input\n",
        "query_input = widgets.Textarea(\n",
        "    placeholder='Type or select a query...',\n",
        "    description='Query:',\n",
        "    layout=widgets.Layout(width='90%', height='120px')\n",
        ")\n",
        "\n",
        "# üîò Submit button\n",
        "submit_button = widgets.Button(\n",
        "    description='Submit',\n",
        "    button_style='success',\n",
        "    tooltip='Submit your query',\n",
        "    icon='search'\n",
        ")\n",
        "\n",
        "# üì¶ Output box\n",
        "output_box = widgets.Output()\n",
        "\n",
        "# üß† Define query execution function\n",
        "def run_query(query_text):\n",
        "    try:\n",
        "        # Strip any leading or trailing whitespace\n",
        "        cleaned_query = query_text.strip()\n",
        "\n",
        "        # Run the query\n",
        "        response = rag_engine.query(cleaned_query).response\n",
        "\n",
        "        # Format the response in a styled box with black text\n",
        "        formatted_response = f\"\"\"\n",
        "        <div style=\"\n",
        "            background-color: #f9f9f9;\n",
        "            border-left: 6px solid #007bff;\n",
        "            padding: 15px;\n",
        "            margin-bottom: 15px;\n",
        "            border-radius: 8px;\n",
        "            font-family: Arial, sans-serif;\n",
        "            color: #000000;  /* Ensure text is black */\n",
        "        \">\n",
        "        <h3>üìù Query:</h3>\n",
        "        <p><strong>{cleaned_query}</strong></p>\n",
        "        <hr>\n",
        "        <h3>üìÑ Response:</h3>\n",
        "        <p>{response}</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        # Display formatted HTML\n",
        "        with output_box:\n",
        "            clear_output()\n",
        "            display(HTML(formatted_response))\n",
        "\n",
        "    except Exception as e:\n",
        "        # Display error in styled box\n",
        "        with output_box:\n",
        "            clear_output()\n",
        "            display(HTML(f\"\"\"\n",
        "            <div style=\"\n",
        "                background-color: #f8d7da;\n",
        "                border-left: 6px solid #dc3545;\n",
        "                padding: 15px;\n",
        "                margin-bottom: 15px;\n",
        "                border-radius: 8px;\n",
        "                font-family: Arial, sans-serif;\n",
        "                color: #000000;  /* Ensure text is black */\n",
        "            \">\n",
        "            <h3>‚ùå Error:</h3>\n",
        "            <p>{str(e)}</p>\n",
        "            </div>\n",
        "            \"\"\"))\n",
        "\n",
        "# üß© Handle prompt selection\n",
        "def on_prompt_change(change):\n",
        "    if change.new in preset_prompts:\n",
        "        query_input.value = preset_prompts[change.new]\n",
        "\n",
        "prompt_dropdown.observe(on_prompt_change, names='value')\n",
        "\n",
        "# üöÄ Handle query submission\n",
        "def on_submit_click(b):\n",
        "    run_query(query_input.value.strip())\n",
        "\n",
        "submit_button.on_click(on_submit_click)\n",
        "\n",
        "# üéØ Display the interactive UI\n",
        "display(widgets.VBox([\n",
        "    prompt_dropdown,\n",
        "    query_input,\n",
        "    submit_button,\n",
        "    output_box\n",
        "]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ctMyXE5I2ATs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
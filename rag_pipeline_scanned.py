# -*- coding: utf-8 -*-
"""RAG Pipeline Scanned.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M5pF2-JCVglSErpGVlop7pvEU5g76ZKJ

### 🔧 Install Required Libraries
This cell installs the necessary packages.
"""

# Install required libraries
!pip install -q llama-index-llms-google-genai llama-index pymupdf
!pip install -q llama-index-embeddings-huggingface
!pip install nest_asyncio
!pip install llama-index-retrievers-bm25
!pip install -q ipywidgets
!apt install tesseract-ocr
!pip install pymupdf pytesseract opencv-python pillow
!pip install sentence-transformers

"""### 🧱 Environment Setup
This cell imports all core libraries and configures the Google Gemini API key, along with some utility setup for directory and display.

"""

import re
import json
import cv2
import pytesseract
import numpy as np
from PIL import Image
import os
import fitz
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import Markdown, display
import nest_asyncio
from google.colab import files
from llama_index.core import Document
from typing import List
from llama_index.core.schema import Document, TextNode
from llama_index.llms.google_genai import GoogleGenAI
from llama_index.core import Settings
from llama_index.core import VectorStoreIndex
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.llms import ChatMessage
from llama_index.core.prompts import ChatPromptTemplate
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SentenceTransformerRerank
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.retrievers import BaseRetriever
from llama_index.core.schema import NodeWithScore, QueryBundle
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter
from llama_index.core.text_splitter import SentenceSplitter
from llama_index.core.node_parser import SimpleNodeParser

nest_asyncio.apply()

# Set up Google API key for Gemini
GOOGLE_API_KEY = "AIzaSyAyNPT5Kl_Y00dOwlAzQ9Y8mzCQSwZN1F4"  # Replace with your actual API key
os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

# Create a directory for our PDFs if it doesn't exist
!mkdir -p sample_docs

"""### ⚙️ RAG Configuration Parameters
All major parameters for chunking, retrieval, reranking, and query expansion are centralized in the `rag_config` dictionary for easier tuning.

"""

# RAG configuration for chunking, retrieval, reranking
rag_config = {
    "fine_chunk_size": 256,
    "fine_chunk_overlap": 20,
    "coarse_chunk_size": 1024,
    "retrieval_top_k": 4,
    "rerank_top_n": 2,
    "num_query_expansions": 3
}

"""### 📄 Upload PDF
This cell allows the user to upload a PDF file that will be used for building the RAG pipeline.

"""

def upload_pdf():
    """Upload a PDF file and return its path."""
    print("Please select a PDF file to upload:")
    uploaded = files.upload()

    for filename in uploaded.keys():
        if filename.endswith('.pdf'):
            # Save to the sample_docs directory
            pdf_path = os.path.join("sample_docs", filename)

            # Create directory if it doesn't exist
            os.makedirs("sample_docs", exist_ok=True)

            # Save the file
            with open(pdf_path, 'wb') as f:
                f.write(uploaded[filename])

            print(f"PDF saved to {pdf_path}")
            return pdf_path
        else:
            print(f"File {filename} is not a PDF. Please upload a PDF file.")

    return None

# upload your own PDF
pdf_path = upload_pdf()

"""### 📘 Load PDF and Extract Text ( PDF Parsing )
Uses PyMuPDF to read each page of the uploaded PDF and convert it into structured LlamaIndex `Document` objects with metadata.

"""

# =======================
# 📌 Convert PDF to Images for OCR
# =======================
def pdf_to_images(pdf_path):
    """
    Convert each page of a PDF file into high-resolution images for OCR.
    Returns a list of images, one per page.
    """
    images = []
    with fitz.open(pdf_path) as doc:
        for page_num in range(len(doc)):
            # Extract each page as a high-quality image
            pix = doc[page_num].get_pixmap()
            img = np.array(Image.frombytes("RGB", [pix.width, pix.height], pix.samples))
            images.append(img)
    return images

# =======================
# 📌 Preprocess Image for OCR
# =======================
def preprocess_image(img, show_preview=False):
    """
    Preprocess an image for OCR by enhancing contrast and sharpening.
    Uses CLAHE for local contrast normalization and Laplacian filtering for edge enhancement.
    """
    # Convert to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

    # Apply CLAHE to enhance local contrast
    clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(8, 8))
    gray = clahe.apply(gray)

    # Apply a mild sharpening filter to enhance edges
    sharpening_kernel = np.array([
        [0, -1, 0],
        [-1, 4.5, -1],
        [0, -1, 0]
    ])
    gray = cv2.filter2D(gray, -1, sharpening_kernel)

    # Resize for better OCR accuracy (Tesseract works better on larger text)
    scale_percent = 200  # Increase image size by 200%
    width = int(gray.shape[1] * scale_percent / 100)
    height = int(gray.shape[0] * scale_percent / 100)
    gray = cv2.resize(gray, (width, height), interpolation=cv2.INTER_CUBIC)

    # Optionally display the preprocessed image
    if show_preview:
        display(Image.fromarray(gray))

    return gray

# =======================
# 📌 Perform OCR on Preprocessed Images
# =======================
def perform_ocr(images):
    """
    Extracts text and bounding box data from a list of preprocessed images.
    Returns extracted text and structured OCR data.
    """
    all_text = []
    all_ocr_data = []
    custom_config = r'--oem 3 -l eng'  # Use Tesseract's LSTM OCR engine

    for gray in images:
        # Extract plain text
        ocr_text = pytesseract.image_to_string(gray, config=custom_config)
        all_text.append(ocr_text)

        # Extract detailed OCR data (bounding boxes, confidence)
        ocr_data = pytesseract.image_to_data(gray, output_type=pytesseract.Output.DICT)
        all_ocr_data.append(ocr_data)

    return all_text, all_ocr_data

# =======================
# 📌 Bounding Box Visualization for OCR Validation
# =======================
def visualize_bounding_boxes(original_image, processed_image, ocr_data, confidence_threshold=30):
    """
    Draws bounding boxes on the preprocessed image for verification.
    Only boxes with confidence above the threshold are displayed.
    """
    # Use a color version of the processed image for drawing
    image_copy = cv2.cvtColor(processed_image, cv2.COLOR_GRAY2BGR)

    for i in range(len(ocr_data['text'])):
        # Extract bounding box and confidence information
        x, y, w, h = ocr_data['left'][i], ocr_data['top'][i], ocr_data['width'][i], ocr_data['height'][i]
        conf = int(ocr_data['conf'][i])

        # Draw only high-confidence boxes
        if conf > confidence_threshold:
            image_copy = cv2.rectangle(image_copy, (x, y), (x + w, y + h), (0, 255, 0), 2)
            text = ocr_data['text'][i].strip()
            if text:
                # Overlay the recognized text near the bounding box
                cv2.putText(image_copy, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1, cv2.LINE_AA)

    # Display the image with bounding boxes
    plt.figure(figsize=(15, 20))
    plt.imshow(cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.show()

# =======================
# 📌 Document Type Detection
# =======================
def is_scanned_pdf(pdf_path, text_threshold=100):
    """
    Detects if a PDF is scanned or well-structured.
    Returns True if the PDF is likely scanned (low text content), False otherwise.
    """
    with fitz.open(pdf_path) as doc:
        total_text = "".join([page.get_text() for page in doc])

    # Consider it scanned if the total extracted text is below the threshold
    return len(total_text.strip()) < text_threshold

# =======================
# 📌 Main Processing Logic (Optimized)
# =======================
def process_and_index_pdf(pdf_path):
    """
    Process a scanned or well-structured PDF and create a vector index with improved handling for structured PDFs.
    """
    # Check if the document is scanned
    is_scanned = is_scanned_pdf(pdf_path)

    # Extract text based on document type
    if is_scanned:
        print("📝 Document Type: Scanned (OCR Required)")
        images = pdf_to_images(pdf_path)
        preprocessed_images = [preprocess_image(img) for img in images]
        ocr_texts, ocr_datasets = perform_ocr(preprocessed_images)
        documents = [Document(text=text) for text in ocr_texts]
    else:
        print("✅ Document Type: Well-Structured (No OCR Needed)")
        documents = []
        with fitz.open(pdf_path) as doc:
            for i, page in enumerate(doc):
                # Extract raw text from each page
                text = page.get_text()

                # Preserve critical newlines for logical chunking
                text = "\n".join([line.strip() for line in text.splitlines() if line.strip()])

                # Create Document object with metadata
                documents.append(Document(
                    text=text,
                    metadata={
                        "file_name": os.path.basename(pdf_path),
                        "page_number": i + 1,
                        "total_pages": len(doc)
                    }
                ))

    # Create vector index
    all_nodes = multi_granularity_chunking(documents, pdf_path=pdf_path)
    vector_index = VectorStoreIndex(all_nodes)
    print(f"✅ Indexed {len(all_nodes)} multi-granularity chunks")

    return vector_index

"""### 🧩 Multi-Granularity Chunking
Splits each document into both fine-grained (sentence-level) and coarse-grained (section/page-level) chunks for better retrieval.

"""

# =======================
# 📌 Multi-Granularity Chunking (Optimized for Structured and Scanned PDFs)
# =======================
def multi_granularity_chunking(documents, pdf_path, text_threshold=100):
    """
    Generate fine, coarse, and logical chunks with separate strategies for structured and scanned PDFs.
    - Retains precise section titles for structured PDFs.
    - Simplifies chunking for scanned PDFs to reduce noise.
    """
    # Check if the document is scanned
    is_scanned = is_scanned_pdf(pdf_path, text_threshold=text_threshold)

    fine_nodes = []
    coarse_nodes = []
    logical_nodes = []

    # Fine-grained chunking (sentence-level)
    sentence_splitter = SentenceSplitter(
        chunk_size=rag_config["fine_chunk_size"],
        chunk_overlap=rag_config["fine_chunk_overlap"]
    )
    for i, doc in enumerate(documents):
        nodes = sentence_splitter.get_nodes_from_documents([doc])
        for node in nodes:
            node.metadata["chunk_type"] = "fine"
            node.metadata["page_number"] = i + 1
        fine_nodes.extend(nodes)

    # Coarse-grained chunking (paragraph-level)
    coarse_parser = SimpleNodeParser.from_defaults(
        chunk_size=rag_config["coarse_chunk_size"]
    )
    for i, doc in enumerate(documents):
        nodes = coarse_parser.get_nodes_from_documents([doc])
        for node in nodes:
            node.metadata["chunk_type"] = "coarse"
            node.metadata["page_number"] = i + 1
        coarse_nodes.extend(nodes)

    # Logical chunking (Structured PDFs)
    if not is_scanned:
        for doc in documents:
            lines = doc.text.split("\n")
            current_chunk = []
            collecting = False
            section_title = None

            for line in lines:
                line_strip = line.strip()

                # Detect known section headers
                if any(anchor in line_strip.upper() for anchor in [
                    "TOTAL ESTIMATED MONTHLY PAYMENT",
                    "FEES WORKSHEET",
                    "ORIGINATION CHARGES",
                    "OTHER CHARGES"
                ]):
                    if current_chunk:
                        chunk_text = "\n".join(current_chunk)
                        logical_nodes.append(TextNode(text=chunk_text, metadata={"section": section_title, "chunk_type": "logical"}))
                        current_chunk = []

                    collecting = True
                    section_title = line_strip
                    current_chunk = [line_strip]
                    continue

                if collecting:
                    if line_strip == "" and current_chunk:
                        chunk_text = "\n".join(current_chunk)
                        logical_nodes.append(TextNode(text=chunk_text, metadata={"section": section_title, "chunk_type": "logical"}))
                        current_chunk = []
                        collecting = False
                    else:
                        current_chunk.append(line_strip)

            # Add remaining chunk
            if current_chunk:
                chunk_text = "\n".join(current_chunk)
                logical_nodes.append(TextNode(text=chunk_text, metadata={"section": section_title, "chunk_type": "logical"}))

    # Logical chunking (Scanned PDFs)
    else:
        for doc in documents:
            lines = doc.text.split("\n")
            current_chunk = []
            for line in lines:
                line_strip = line.strip()
                if line_strip:
                    current_chunk.append(line_strip)
            if current_chunk:
                logical_nodes.append(TextNode(
                    text="\n".join(current_chunk),
                    metadata={"chunk_type": "logical"}
                ))

    # Print summary for verification
    print(f"✅ Final Chunk Counts - Fine: {len(fine_nodes)}, Coarse: {len(coarse_nodes)}, Logical: {len(logical_nodes)}")

    return fine_nodes + coarse_nodes + logical_nodes

"""### 📚 Embedding and Index Construction
Initializes the Gemini LLM and HuggingFace embedding model, then builds a vector index from multi-granularity chunks.

"""

# Set up the custom prompt for the Gemini LLM
custom_prompt = ChatPromptTemplate.from_messages([
    ("system", (
        "You are a highly skilled assistant specializing in analyzing mortgage and loan-related documents, "
        "such as Lender Fee Worksheets and Loan Estimates.\n\n"
        "Your task is to accurately extract and reason over the content retrieved from these documents. "
        "Always rely on the retrieved context only — do not assume or hallucinate any values or terms.\n\n"
        "When answering:\n"
        "- Be precise with all numerical values, dates, and percentages.\n"
        "- When totals are requested, check if they need to be calculated from individual components like taxes, insurance, and principal.\n"
        "- If a query implies composition or reasoning (e.g., total payment, remaining balance), compute carefully and explain briefly if needed.\n"
        "- If the information is not in the retrieved content, respond clearly that it was not found.\n"
        "- Use mortgage-specific terminology appropriately and avoid ambiguity.\n\n"
        "You are being used in a legal or financial setting where accuracy and clarity are critical."
    )),
    ("user", "{query_str}")
])

# Initialize the Gemini LLM
llm = GoogleGenAI(
    model="models/gemini-2.0-flash",
    prompt=custom_prompt,
    max_output_tokens=1024  # Adjust based on your use case
)

# Initialize the HuggingFace embedding model
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")  # or "intfloat/e5-base-v2"
Settings.embed_model = embed_model

"""### 🚀 Index the Uploaded PDF
Calls the full document processing and indexing function to prepare for RAG-based querying.

"""

# =======================
# 📌 Run the Indexing
# =======================
index = process_and_index_pdf(pdf_path)

"""### 🧠 Full RAG Pipeline Builder
Constructs the complete Retrieval-Augmented Generation engine with hybrid retrieval (semantic + keyword), query expansion, and reranking.

"""

def build_rag_pipeline(index):
    """Build a RAG pipeline with hybrid retrieval, query expansion, and reranking."""

    # Get all nodes from the index's docstore
    nodes = list(index.docstore.docs.values())
    num_nodes = len(nodes)

    # Set top_k based on configuration
    safe_top_k = min(rag_config["retrieval_top_k"], max(1, num_nodes))
    print(f"Index contains {num_nodes} nodes, using top_k={safe_top_k}")

    # Step 1: Define Filters for Each Retrieval Method
    filter_by_fine_type = MetadataFilters(filters=[ExactMatchFilter(key="chunk_type", value="fine")])
    filter_by_coarse_type = MetadataFilters(filters=[ExactMatchFilter(key="chunk_type", value="coarse")])
    filter_by_page_level = MetadataFilters(filters=[ExactMatchFilter(key="chunk_type", value="page")])

    # Step 2: Create Individual Retrievers
    vector_retriever = index.as_retriever(similarity_top_k=safe_top_k, filters=filter_by_fine_type)
    bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=safe_top_k)
    page_retriever = index.as_retriever(similarity_top_k=safe_top_k, filters=filter_by_page_level)

    # Step 3: Hybrid Retriever for Combining Methods
    class HybridRetriever(BaseRetriever):
        def __init__(self, vector_retriever, bm25_retriever, page_retriever, top_k):
            self.vector_retriever = vector_retriever
            self.bm25_retriever = bm25_retriever
            self.page_retriever = page_retriever
            self.top_k = top_k
            super().__init__()

        def _retrieve(self, query_bundle, **kwargs):
            vector_nodes = self.vector_retriever.retrieve(query_bundle)
            bm25_nodes = self.bm25_retriever.retrieve(query_bundle)
            page_nodes = self.page_retriever.retrieve(query_bundle)

            # Combine all retrieved nodes
            all_nodes = list(vector_nodes) + list(bm25_nodes) + list(page_nodes)

            # Remove duplicates by node_id
            unique_nodes = {}
            for node in all_nodes:
                if node.node_id not in unique_nodes:
                    unique_nodes[node.node_id] = node

            # Sort nodes by score, highest first
            sorted_nodes = sorted(
                unique_nodes.values(),
                key=lambda x: x.score if hasattr(x, "score") and x.score else 0.0,
                reverse=True
            )

            # Return top_k results
            return sorted_nodes[:self.top_k]

    # Initialize Hybrid Retriever
    hybrid_retriever = HybridRetriever(
        vector_retriever=vector_retriever,
        bm25_retriever=bm25_retriever,
        page_retriever=page_retriever,
        top_k=safe_top_k
    )

    # Step 4: Reranker (Optional)
    node_postprocessors = []
    if num_nodes > 1:
        reranker = SentenceTransformerRerank(
            model="cross-encoder/ms-marco-MiniLM-L-12-v2",
            top_n=min(rag_config["rerank_top_n"], num_nodes)
        )
        node_postprocessors.append(reranker)

    # Step 5: Query Expansion with Fusion Retriever
    fusion_retriever = QueryFusionRetriever(
        retrievers=[hybrid_retriever],
        llm=llm,
        num_queries=rag_config["num_query_expansions"],
        similarity_top_k=safe_top_k,
        mode="reciprocal_rerank"
    )

    # Step 6: Final Query Engine
    query_engine = RetrieverQueryEngine.from_args(
        retriever=fusion_retriever,
        llm=llm,
        node_postprocessors=node_postprocessors
    )

    print(f"✅ Hybrid RAG Pipeline built successfully with {num_nodes} nodes.")

    def run_query_with_reranking(query_text, top_k=4):
        query_bundle = QueryBundle(query_str=query_text)
        nodes = hybrid_retriever._retrieve(query_bundle)
        reranked_nodes = reranker.postprocess_nodes(nodes, query_str=query_text) if reranker else nodes

        # Prepare DataFrame for comparison
        results = []

        # Combined Results for Comparison
        for i, node in enumerate(nodes):
            results.append({
                "Stage": "Original Retrieval",
                "Rank": i + 1,
                "Score": node.score,
                "Content": node.get_text()[:150] + "...",
                "Page": node.metadata.get("page_number", "Unknown")
            })

        for i, node in enumerate(reranked_nodes):
            results.append({
                "Stage": "After Reranking",
                "Rank": i + 1,
                "Score": node.score,
                "Content": node.get_text()[:150] + "...",
                "Page": node.metadata.get("page_number", "Unknown")
            })

        # Display the results in a clean table
        results_df = pd.DataFrame(results)
        display(results_df)

    return query_engine, run_query_with_reranking

# Rebuild the RAG pipeline
rag_engine, run_query_with_reranking = build_rag_pipeline(index)

# Example usage of the integrated reranking demonstration
run_query_with_reranking("What is the total estimated monthly payment?", top_k=4)

"""### 🔍 Run a Sample Query
This cell queries the final RAG engine to answer a question using the processed PDF content.

"""

# 🧰 Imports
import ipywidgets as widgets
from IPython.display import display, clear_output, HTML

# 📂 Ensure the index is processed correctly
index = process_and_index_pdf(pdf_path)
rag_engine, run_query_with_reranking = build_rag_pipeline(index)

# 📋 Preset prompts
preset_prompts = {
    "Who is the mortgagor?": "Who is the mortgagor in the document?",
    "Borrower's owes": "How much does the borrower owes the lender?",
    "Uniform Covenants": "What are the Uniform Covenants in detail?",
    "Parcel Identifier number": "What is the parcel identifier number?",
    "Document number": "What is the document number?",
    "what are escrow items?": "what are escrow items?"

}

# 🔽 Dropdown for sample prompts
prompt_dropdown = widgets.Dropdown(
    options=["Select a sample prompt"] + list(preset_prompts.keys()),
    description='Examples:',
    layout=widgets.Layout(width='90%')
)

# 📝 Multiline text input
query_input = widgets.Textarea(
    placeholder='Type or select a query...',
    description='Query:',
    layout=widgets.Layout(width='90%', height='120px')
)

# 🔘 Submit button
submit_button = widgets.Button(
    description='Submit',
    button_style='success',
    tooltip='Submit your query',
    icon='search'
)

# 📦 Output box
output_box = widgets.Output()

# 🧠 Define query execution function
def run_query(query_text):
    try:
        # Strip any leading or trailing whitespace
        cleaned_query = query_text.strip()

        # Run the query
        response = rag_engine.query(cleaned_query).response

        # Format the response in a styled box with black text
        formatted_response = f"""
        <div style="
            background-color: #f9f9f9;
            border-left: 6px solid #007bff;
            padding: 15px;
            margin-bottom: 15px;
            border-radius: 8px;
            font-family: Arial, sans-serif;
            color: #000000;  /* Ensure text is black */
        ">
        <h3>📝 Query:</h3>
        <p><strong>{cleaned_query}</strong></p>
        <hr>
        <h3>📄 Response:</h3>
        <p>{response}</p>
        </div>
        """

        # Display formatted HTML
        with output_box:
            clear_output()
            display(HTML(formatted_response))

    except Exception as e:
        # Display error in styled box
        with output_box:
            clear_output()
            display(HTML(f"""
            <div style="
                background-color: #f8d7da;
                border-left: 6px solid #dc3545;
                padding: 15px;
                margin-bottom: 15px;
                border-radius: 8px;
                font-family: Arial, sans-serif;
                color: #000000;  /* Ensure text is black */
            ">
            <h3>❌ Error:</h3>
            <p>{str(e)}</p>
            </div>
            """))

# 🧩 Handle prompt selection
def on_prompt_change(change):
    if change.new in preset_prompts:
        query_input.value = preset_prompts[change.new]

prompt_dropdown.observe(on_prompt_change, names='value')

# 🚀 Handle query submission
def on_submit_click(b):
    run_query(query_input.value.strip())

submit_button.on_click(on_submit_click)

# 🎯 Display the interactive UI
display(widgets.VBox([
    prompt_dropdown,
    query_input,
    submit_button,
    output_box
]))

